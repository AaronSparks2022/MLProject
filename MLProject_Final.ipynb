{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51cd8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "df = pd.read_csv(\"ai_human_content_detection_dataset.csv\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Embedding function for raw text\n",
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name=\"intfloat/e5-base-v2\", batch_size=32):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.model is None:\n",
    "            self.model = SentenceTransformer(self.model_name, device=self.device)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a 1D vector of text\n",
    "        if hasattr(X, \"iloc\"):            # pandas object (Series or DataFrame)\n",
    "            X = X.iloc[:, 0] if X.ndim > 1 else X\n",
    "        texts = X.astype(str).tolist()\n",
    "\n",
    "        emb = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=False,\n",
    "            device=self.device\n",
    "        )\n",
    "        return emb\n",
    "\n",
    "# Early train-test split to prevent data leakage\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# Sentence and word length variance\n",
    "train_df[\"sent_len_variance\"] = train_df[\"text_content\"].apply(lambda t: np.var([len(s.split()) for s in t.split('.') if len(s.split())>0]))\n",
    "train_df[\"word_len_variance\"] = train_df[\"text_content\"].apply(lambda t: np.var([len(w) for w in t.split()]) if len(t.split())>0 else 0)\n",
    "test_df[\"sent_len_variance\"] = test_df[\"text_content\"].apply(lambda t: np.var([len(s.split()) for s in t.split('.') if len(s.split())>0]))\n",
    "test_df[\"word_len_variance\"] = test_df[\"text_content\"].apply(lambda t: np.var([len(w) for w in t.split()]) if len(t.split())>0 else 0)\n",
    "\n",
    "# Repeated words\n",
    "def rep_ratio(text):\n",
    "    words = text.lower().split()\n",
    "    return len(words) - len(set(words))\n",
    "train_df[\"repeat_words\"] = train_df[\"text_content\"].apply(rep_ratio)\n",
    "test_df[\"repeat_words\"] = test_df[\"text_content\"].apply(rep_ratio)\n",
    "\n",
    "# Model selection and pre-encoding for faster CV later down pipeline\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\", device=device)\n",
    "train_emb = model.encode(\n",
    "    train_df[\"text_content\"].astype(str).tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "test_emb = model.encode(\n",
    "    test_df[\"text_content\"].astype(str).tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# np.save(\"embeddings_train.npy\", train_emb)\n",
    "# np.save(\"embeddings_test.npy\", test_emb)\n",
    "\n",
    "# #emb = np.load(\"embeddings_mpnet.npy\")\n",
    "# # Remove duplicate embedding columns\n",
    "\n",
    "train_emb_df = pd.DataFrame(train_emb, columns=[f\"emb_{i}\" for i in range(train_emb.shape[1])])\n",
    "test_emb_df = pd.DataFrame(test_emb, columns=[f\"emb_{i}\" for i in range(test_emb.shape[1])])\n",
    "\n",
    "train_df = pd.concat([train_df.reset_index(drop=True), train_emb_df], axis=1)\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_emb_df], axis=1)\n",
    "\n",
    "X_train = train_df.drop(columns=[\"label\", \"text_content\"])\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"label\", \"text_content\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "\n",
    "# Select only base useful numeric and categorical columns\n",
    "numeric = [\n",
    "    'word_count', 'avg_word_length', 'avg_sentence_length',\n",
    "    'flesch_reading_ease', 'gunning_fog_index',\n",
    "    'grammar_errors', 'punctuation_count', 'sent_len_variance', 'word_len_variance', \n",
    "    'repeat_words'\n",
    "]\n",
    "\n",
    "categorical = ['content_type']\n",
    "\n",
    "# Ensure only existing columns are selected \n",
    "numeric = [col for col in numeric if col in X_train.columns]\n",
    "categorical = [col for col in categorical if col in X_train.columns]\n",
    "embedding_cols = [c for c in X_train.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "\n",
    "# Imputer, Scaler, and One-Hot-Encoding \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Embedding pipeline \n",
    "embedding_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True)),\n",
    "])\n",
    "\n",
    "# Your stylometric pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric),\n",
    "        (\"cat\", categorical_transformer, categorical),\n",
    "        (\"emb\", embedding_transformer, embedding_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Model Definition\n",
    "pipeline = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=1.0\n",
    "    ))\n",
    "])\n",
    "\n",
    "# RFC parameter distributions for use in the RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"clf__C\": loguniform(1e-4, 1e2),\n",
    "    \"clf__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "    \"clf__max_iter\": [2000, 5000, 8000]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Params:\", search.best_params_)\n",
    "print(\"Best CV Accuracy:\", search.best_score_)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plots and Data\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Human\", \"AI\"], yticklabels=[\"Human\", \"AI\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# PCA Plot\n",
    "pca = PCA(n_components=2)\n",
    "xy = pca.fit_transform(np.vstack((train_emb, test_emb)))\n",
    "\n",
    "labels_all = np.concatenate([y_train, y_test])\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(xy[:,0], xy[:,1], c=labels_all, cmap=\"coolwarm\", alpha=0.6)\n",
    "plt.title(\"PCA Projection of E5 Embeddings\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(label=\"Label (0=Human, 1=AI)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Importance Plot\n",
    "feature_names = numeric  # your stylometric features\n",
    "coef = np.abs(best_model.named_steps[\"clf\"].coef_[0][:len(feature_names)])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=coef, y=feature_names)\n",
    "plt.title(\"LinearSVC: Stylometric Feature Influence\")\n",
    "plt.xlabel(\"Absolute Weight\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "# Embedding Variance\n",
    "human_emb = train_emb[y_train == 0]\n",
    "ai_emb = train_emb[y_train == 1]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(human_emb.var(axis=0), label=\"Human Text\")\n",
    "plt.plot(ai_emb.var(axis=0), label=\"AI Text\")\n",
    "plt.title(\"Variance Across Embedding Dimensions\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.clustermap(np.vstack((train_emb[:200], test_emb[:200])), cmap=\"viridis\")\n",
    "\n",
    "pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
